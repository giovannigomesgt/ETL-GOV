from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator, BranchPythonOperator
import os
import requests
from zipfile import ZipFile
import re
from bs4 import BeautifulSoup
from airflow.operators.dagrun_operator import TriggerDagRunOperator
from datetime import datetime, timedelta
from airflow.utils.trigger_rule import TriggerRule
from os import walk
from airflow.models import Variable
import boto3
from pytz import timezone
from urllib.parse import urljoin

import botocore.exceptions


from airflow.providers.amazon.aws.hooks.s3 import S3Hook

## 256240406578-datalake-
## 256240406578-datalake-

# Variables

ENVIRONMENT = Variable.get("ENVIRONMENT_GOV")
BUCKET_NAME = f'256240406578-datalake-{ENVIRONMENT}-raw'
RF_URL = 'https://dadosabertos.rfb.gov.br/CNPJ/'

# Cria um objeto S3Hook
s3_hook = S3Hook(aws_conn_id='aws_default')

def getLinks():
    url = 'http://200.152.38.155/CNPJ/'
    soup = BeautifulSoup(requests.get(url).content, 'html.parser')
    links = [urljoin(url, link.get('href'))
             for link in soup.find_all('a', href=re.compile('\.zip$'))]
    for link in links:
        print(link)
    return links
   
def cnaes(task_instance):
    links = task_instance.xcom_pull(task_ids='Get_Links')
    filescnaes = [lista for lista in links if 'cnaes' in lista.lower()]
    return filescnaes

#### VERSIONAMENTO ####

def versioning2(task_instance):
    file = task_instance.xcom_pull(task_ids='Find_Cnaes')
    objectname = file[0].split('/')[-1].replace('zip', 'CSV')

    # Caminho do objeto no S3
    object_name = f'dados_publicos_cnpj/Cnaes/{objectname}'

    try:
        # Obtém informações do objeto
        response = s3_hook.get_key(object_name, BUCKET_NAME)

        # Imprime a data da última modificação
        last_modified = response.last_modified.replace(tzinfo=None)

        
    except Exception as e:
        if 'AccessDenied' in str(e):
            # tratamento de exceção específico para permissões insuficientes
            print(f'Não foi possível acessar o bucket de {ENVIRONMENT} devido a permissões insuficientes')
            return 'fim'

        elif '404' in str(e):
            print('Arquivo não encontrado')
            return 'downloading'
        else:
            raise

    page = requests.get(RF_URL)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find('table')
    versao = []
    for row in table.find_all('tr'):
        cells = row.find_all('td')
        if cells:
            if '.zip' in cells[1].text:
                versao.append({
                    'file': cells[1].text.replace(' ', ''),
                    'version': cells[2].text.replace('  ', '')
                })

    for i in versao:
        if file[0].split('/')[-1] in i['file']:
            date_object = datetime.strptime(i['version'], "%Y-%m-%d %H:%M")

            if date_object > last_modified:
                print('*' * 100)
                print(f'Data no S3 {last_modified}')
                print(f'Última publicação no Gov {date_object}')
                print('*' * 100)
                return 'downloading'
            else:
                print('*' * 100)
                print(f'Data no S3 {last_modified}')
                print(f'Última publicação no Gov {date_object}')
                print('*' * 100)
                return 'version_ok'


def versioning(task_instance):
    file = task_instance.xcom_pull(task_ids='Find_Cnaes')
    objectname = file[0].split('/')[-1].replace('zip', 'CSV')

    # Caminho do objeto no S3
    object_name = f'dados_publicos_cnpj/Cnaes/{objectname}'

    try:
        # Obtém informações do objeto
        response = s3_hook.get_key(object_name, BUCKET_NAME)

        # Imprime a data da última modificação
        last_modified = response.last_modified.replace(tzinfo=timezone('UTC'))
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == 'AccessDenied':
            # tratamento de exceção específico para permissões insuficientes
            print(f'Não foi possível acessar o bucket de {ENVIRONMENT} devido a permissões insuficientes')
            return 'fim'

        elif e.response['Error']['Code'] == "404":
            print('Arquivo não encontrado')
            return 'Downloading'
        else:
            raise

    url = 'https://dadosabertos.rfb.gov.br/CNPJ/'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    table = soup.find('table')
    versao = []
    for row in table.find_all('tr'):
        cells = row.find_all('td')
        if cells:
            if '.zip' in cells[1].text:
                versao.append({
                    'file': cells[1].text.replace(' ', ''),
                    'version': cells[2].text.replace('  ', '')
                })

    for i in versao:
        if file[0].split('/')[-1] in i['file']:
            date_object = datetime.strptime(i['version'], "%Y-%m-%d %H:%M")
            date_object = date_object.replace(tzinfo=timezone('UTC'))

            if date_object > last_modified:
                print('*' * 100)
                print(f'Data no S3 {last_modified}')
                print(f'Última publicação no Gov {date_object}')
                print('*' * 100)
                return 'Downloading'
            else:
                print('*' * 100)
                print(f'Data no S3 {last_modified}')
                print(f'Última publicação no Gov {date_object}')
                print('*' * 100)
                return 'Version_OK'
   
def download(task_instance):
    file_urls = task_instance.xcom_pull(task_ids='Find_Cnaes')
    for file_url in file_urls:
        response = requests.head(file_url)
        file_name = os.path.basename(file_url)
        folder_name = os.path.splitext(file_name)[0]

        endereco = re.sub(r'\d+', '', folder_name)
        os.makedirs(endereco, exist_ok=True)

        checkfiles = os.listdir(endereco)
        if f"{folder_name}.zip" in checkfiles or f"{folder_name}.CSV" in checkfiles:
            print('Arquivo já existe')
        else:
            response = requests.get(file_url, stream=True)
            print('*' * 100)
            print(f"Downloading {file_name}")
            print('*' * 100)
            with open(f'{endereco}/{file_name}', "wb") as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            print('*' * 100)
            print('Download Concluido!')
            print('*' * 100)
    return f'{endereco}/{file_name}'
    
def skip():
    print('*'*50)
    print('A versão do arquivo está ok')
    print('*'*50)
    return 'fim'

def extract(task_instance):
    # RECEBE O NOME DA PASTA
    folder = task_instance.xcom_pull(task_ids='Find_Cnaes')[0].split('/')[-1].replace('.zip','')
    #nomedoarquivo = folder.split('/')[-1].replace('.zip','')
    folder = re.sub(u'[0123456789]', '',folder)
    print('*'*150)
    print('Iniciando Extração')
    # VERIFICA A EXISTÊNCIA DA PASTA
    print('*'*150)
    if os.path.exists(folder):
        checkfile = os.listdir(folder)
        for file in checkfile:
            if '.zip' in file:
                filename = f'{folder}/{file}'
                print(filename)
                # Abre o arquivo ZIP
                with ZipFile(filename, "r") as z:
                    # Imprime o nome de cada arquivo dentro do arquivo ZIP
                    for compressed in z.namelist():
                        compressedfile = compressed
                    z.extractall(folder)
                    print(f'{filename} extraído')
                os.remove(filename)
                print(f'{filename} excluído')
                nomedoarquivo = filename.split('/')[-1].replace('.zip','')
            #print(f'caminho_do_arquivo {folder}')
            #print(f'Caminho do novo arquivo {nomedoarquivo}')     
                try:
                    # Renomeando Arquivo
                    caminho_do_arquivo = f'{folder}/{compressedfile}'
                    novo_nome = f'{folder}/{nomedoarquivo}.CSV'
                    os.rename(caminho_do_arquivo, novo_nome)
                    print(f'Arquivo {compressedfile} renomeado para {nomedoarquivo}.CSV')
                except:
                    print('arquivo já Existe')
    else:
        print('*'*50)
        print('Pasta Não encontrada')
        print('*'*50)
    print('*'*50)
    return 'Upload_to_S3'

def uploadS3(task_instance):
    # RECEBE O NOME DA PASTA
    file = task_instance.xcom_pull(task_ids='Find_Cnaes')[0].split('/')[-1].replace('.zip','')
    file = re.sub(u'[0123456789]', '',file)

    # VERIFICA A EXISTÊNCIA DA PASTA
    if os.path.exists(file):
        checkfile = os.listdir(file)
        for diretorio, subpasta, arquivos in walk(file):
            for arquivo in arquivos:
                print('*'*150)
                print(f'ARQUIVO ATUAL: {arquivo}')
                pastas3 = diretorio.split('\\')[0].replace('.','').replace('/','')
                pathlocal = f'{diretorio}/{arquivo}'
                paths3 = f'dados_publicos_cnpj/{pastas3}/{arquivo}'

                local_path = pathlocal
                s3_key = paths3

                
                #print(pathlocal)
                print(f"Enviando {pathlocal} para o bucket {BUCKET_NAME} endereço da pasta: {paths3}")
                
                s3_hook.load_file(
                    filename=local_path,
                    key=s3_key,
                    BUCKET_NAME=BUCKET_NAME,
                    replace=True
                )
                print(f'Arquivo {arquivo} Enviado!')
                print(f'Excluindo ./{pathlocal}')
                os.remove(f'./{pathlocal}')
                print('*'*150)
    else:
        pass
    checkfile = os.listdir(file)
    print('*'*150)
    print('Itens na pasta:')
    for files in checkfile: # LISTA OS ARQUIVOS NA PASTA
        print(f'{files}')
        #os.remove(files)
    print('*'*150)
 

with DAG('teste2', start_date=datetime(2022,12,16), schedule_interval=None, catchup= False, tags=['TREINAMENTO','GOV']) as dag:
    
    fim =  DummyOperator(task_id = "fim", trigger_rule=TriggerRule.NONE_FAILED)
        
    taskgetLinks = PythonOperator(
        task_id = 'Get_Links',
        python_callable = getLinks
    )

    taskdownloadcnaes = PythonOperator(
        task_id = 'Find_Cnaes',
        python_callable = cnaes
    )

    taskversioning = BranchPythonOperator(
        task_id = 'Versioning',
        python_callable = versioning2
    )

    taskdownload = PythonOperator(
        task_id = 'Downloading',
        python_callable = download
    )

    taskskip = BranchPythonOperator(
    task_id = 'Version_OK',
    python_callable = skip
    )

    taskextract = BranchPythonOperator(
        task_id='Extracting',
        python_callable=extract,
        #trigger_rule = TriggerRule.ALL_DONE,
        trigger_rule = TriggerRule.NONE_FAILED
    )

    tasks3 = PythonOperator(
        task_id='Upload_to_S3',
        python_callable=uploadS3,
        trigger_rule = TriggerRule.ALL_SUCCESS
    )

    trigger_task = TriggerDagRunOperator(
        task_id='trigger_target_dag',
        trigger_dag_id='de_motivos'
    )

# ORQUESTRAÇÃO

taskgetLinks >> taskdownloadcnaes
taskdownloadcnaes >> taskversioning
taskversioning >> [taskdownload, taskskip]
[taskdownload, taskskip] >> taskextract
taskextract >> tasks3
tasks3 >> fim
fim >> trigger_task


